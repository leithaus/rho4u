\documentclass{article}
\usepackage{amsmath, amssymb, hyperref}
\usepackage{graphicx}
\DeclareUnicodeCharacter{2211}{\ensuremath{\Sigma}}

\makeatletter
\newcommand{\bigplus}{%
  \DOTSB\mathop{\mathpalette\mattos@bigplus\relax}\slimits@
}
\newcommand\mattos@bigplus[2]{%
  \vcenter{\hbox{%
    \sbox\z@{$#1\sum$}%
    \resizebox{!}{0.9\dimexpr\ht\z@+\dp\z@}{\raisebox{\depth}{$\m@th#1+$}}%
  }}%
  \vphantom{\sum}%
}
\makeatother

\newcommand{\mycomment}[1]{}

\title{MeTTa Architecture Proposal}
\author{}
\date{}

\begin{document}

\maketitle

\section{Introduction}

MeTTa (Meta Type Talk) is a proposed language for AI programming. We propose some solutions for some of its problems.

\subsection{Early MeTTa implementations}

\subsection{Problems}

\subsubsection{Leaking Secrets}

\subsubsection{All paths}

Given a nondeterministic language, the current implementations of MeTTa explore all possible reductions instead of choosing one at random, resulting in an exponential explosion in runtime under very weak assumptions.

\subsection{GSLTs as a way forward}

A graph-structured lambda theory (GSLT) is a construction in category theory for specifying the semantics of virtual machines.  Such theories have a bunch of useful features, like automatically handling variable binding and alpha equivalence, including similarity to existing systems like K-framework and Maude.

\subsubsection{Well-defined semantics}

By standardizing on GSLTs, MeTTa users will be able to specify exactly the syntax and semantics they're hoping to implement, as well as be given a type system for their virtual machine.

\subsubsection{Compiles to Rholang}

We outline below a strategy for automatic compilation of GSLTs to Rholang.

\subsubsection{Rholang is optimal}

We prove that any other generic solution must be within a constant factor of the performance of Rholang.

\section{Theories of Operational Semantics}

Our approach to operational semantics is to define a category $\mathsf{Th}$ whose generating objects are \emph{sorts}---the basic atoms out of which the state of the machine is assembled---and whose generating morphisms are term constructors that produce terms of those sorts.

Bill Lawvere pioneered the use of categories to capture grammars modulo equations for structural congruence, so such categories are often called \emph{Lawvere theories}. Lawvere proved that his theories correspond to finitary monads. Our theories are Lawvere theories equipped with extra structure and stuff that allow us to talk about reductions between terms, bind variables, and define modal types.  Specifically, they are \emph{graph-structured interactive multisorted lambda theories}, or GSLTs for short.

\begin{itemize}
    \item {\bf Multisorted.}  Lawvere only considered categories with one generating sort. We need at least two generating sorts, one for terms representing processes, and another for terms representing rewrites between processes. Todd Trimble gave a definition of multisorted Lawvere theories on the nLab
% https://ncatlab.org/toddtrimble/published/multisorted+Lawvere+theories
and proved a similar monadicity theorem.

    \item {\bf Graph-structured.}  We distinguish one of these sorts, say $P$, as the sort of complete machine states, which we call processes. Rewrites are represented as a different sort $R$ equipped with source and target maps from $R$ to $P$. Let $\mathsf{Th(Gph)}$ be the category with two objects and two parallel morphisms between them. A graph-structured theory ${\rm Th}$ is one equipped with a functor from ${\rm Th}({\rm Gph})$ to ${\rm Th}$.

    \item {\bf Interactive.}  We restrict our attention to those programming languages where any base case rewrite rule---that is, any rewrite rule not constructed from another rewrite---is an occurrence of a binary process constructor $\odot,$ which we call the ``interaction''.  For example:

\begin{itemize}
    \item In lambda calculus, beta reduction is of the form $((\lambda x.T) U) \to T[U/x]$, whose source is an application of one process to another.  Here, $\odot$ is application.
    \item In pi calculus, the comm rule is of the form $x?(y).P \mid x!(z) \to P[z/y]$, whose source is a juxtaposition of two processes.  Here, $\odot$ is juxtaposition.
\end{itemize}

Here's an example of a rewrite where $\odot$ is not the top level term constructor: in lambda calculus, a reduction may occur in the head position of an application term.  We have a rewrite that says given any rewrite $r$ and any term $x$, $s((r x)) = (s(r) x)$ and $t((r x)) = (t(r) x).$  Because these rewrites are constructed from $r,$ they are not subject to the requirement to have $\odot$ at the top level.

    \item {\bf Modalities from the interaction.}  Consider the type $[\odot([], A)] B$ of terms that when placed into a term context $\odot([], x)$ with any $x\colon A$ must reduce to a term of type $B$. In lambda calculus, this corresponds to the arrow type $A \to B$. In pi calculus, this is Caires' rely-guarantee type $A \triangleright B$.
% Caires, Luis. "Logical Semantics of Types for Concurrency". Algebra and Coalgebra in Computer Science, Second International Conference, CALCO 2007, Bergen, Norway, August 20-24, 2007, Proceedings
If $B$ can depend on $x$, these are dependent product types.  We can also define the dual notion of a possibility modality.

    \item {\bf Lambda theories.}  Lawvere's theories have finite products. While it is possible to define the machinery of bound variables and substitution in a Lawvere theory, it is a hassle and not particularly interesting. Instead, we use cartesian closed categories, which handle bound variables and substitution automatically.  Our lambda theories are multisorted Lawvere theories with a hom-product adjunction
$${\rm Th}(A \times B, C) \cong {\rm Th}(B, C^A).$$

\end{itemize}

\section{Presentation of finitely generated GSLTs}

A presentation of a GSLT consists of:

\begin{itemize}
    \item A finite set of \textbf{generating sorts}, including two distinguished sorts $P$ (processes) and $R$ (rewrites) from the graph structure.
    \item A choice of a distinguished binary morphism $\odot : X \times Y \to P$, called the \textbf{interaction}, for some pair of sorts $X, Y$.
    \item A finite set of \textbf{generating morphisms}, including the two built-in \textbf{graph structure morphisms} $s, t : R \to P$. Generating morphisms with codomain $P$ are \textbf{term constructors}, and those with codomain $R$ are \textbf{rewrite constructors}:
    \[
    r : \prod_i Z_i \to R
    \]
    If the domain does not contain a factor of $R$, it must factor through the interaction:
    \[
    \exists f : \prod_i Z_i \to X \times Y. \quad \odot \circ f = s \circ r
    \]
    \item A finite set of \textbf{equations} between morphisms. We often write $r : A \leadsto B$ as shorthand for $s(r(z)) = A$, $t(r(z)) = B$.
\end{itemize}

Rewrite term constructors usually come in two flavors:

\begin{itemize}
    \item \textbf{Top-level rewrites} whose domain does not contain a factor of $R$.
    \item \textbf{In-context rewrites} whose domain contains a factor of $R$.
\end{itemize}

\subsection{Examples}
\subsubsection{WHNF $\lambda-$calculus}

\begin{itemize}
    \item \textbf{Two generating sorts} $P, R$
    \item \textbf{Generating morphisms:}
    \begin{itemize}
        \item Graph structure morphisms: $s, t : R \to P$
        \item Term constructors: 
        \begin{itemize}
            \item ${\tt App} : P \times P \to P$
            \item ${\tt Lam} : (P \to P) \to P$
        \end{itemize}
        \item Rewrite constructors:
        \begin{itemize}
            \item ${\tt Beta} : (P \to P) \times P \to R$
            \item ${\tt Head} : R \times P \to R$
        \end{itemize}
    \end{itemize}
    \item \textbf{Interaction:} ${\tt App}$
    \item \textbf{Equations:}
    \begin{itemize}
        \item ${\tt Beta}(A, B) : {\tt App}({\tt Lam}(A), B) \leadsto ev(A, B)$ (top-level)
        \item ${\tt Head}(A, B) : {\tt App}(s(A), B) \leadsto {\tt App}(t(A), B)$ (in-context)
    \end{itemize}
\end{itemize}

\subsubsection{RHO calculus}

\begin{itemize}
    \item \textbf{Three generating sorts} $P, R, N$
    \item \textbf{Generating morphisms:}
    \begin{itemize}
        \item Graph structure morphisms: $s, t : R \to P$
        \item Term constructors: 
        \begin{itemize}
            \item ${\tt Zero} : 1 \to P$
            \item ${\tt Par} : P \times P \to P$
            \item ${\tt Send} : N \times P \to P$
            \item ${\tt For} : N \times (N \to P) \to P$
            \item ${\tt Drop} : N \to P$
            \item ${\tt Quote} : P \to N$
        \end{itemize}
        \item Rewrite constructors:
        \begin{itemize}
            \item ${\tt Comm} : N \times (N \to P) \times P \to R$
            \item ${\tt Par}_1 : R \times P \to R$
            \item ${\tt Par}_2 : R \times R \to R$
        \end{itemize}
    \end{itemize}
    \item \textbf{Interaction:} ${\tt Par}$
    \item \textbf{Equations:}
    \begin{itemize}
        \item Commutative monoid laws
            \begin{itemize}
                \item ${\tt Par}({\tt Zero}, Q) = Q$
                \item ${\tt Par}(T, U) = {\tt Par}(U, T)$
                \item ${\tt Par}({\tt Par}(T, U), V) = {\tt Par}(T, {\tt Par}(U, V))$
            \end{itemize}
        \item Quotation laws
            \begin{itemize}
                \item ${\tt Quote}({\tt Drop}(x)) = x$
                \item ${\tt Drop}({\tt Quote}(Q)) = Q$ (but see section \ref{TypedGSLTs})
            \end{itemize}
        \item Communication
            \begin{itemize}
                \item ${\tt Comm}(x, K, Q) : {\tt Par}({\tt For}(x, K), {\tt Send}(x, Q)) \leadsto ev(K, @Q)$
            \end{itemize}
        \item Par contexts
            \begin{itemize}
                \item ${\tt Par}_1(r, Q) : {\tt Par}(s(r), Q) \leadsto {\tt Par}(t(r), Q)$
                \item ${\tt Par}_2(r_1, r_2) : {\tt Par}(s(r_1), s(r_2)) \leadsto {\tt Par}(t(r_1), t(r_2))$
            \end{itemize}
    \end{itemize}
\end{itemize}

\section{Generating typed GSLTs from untyped ones}
\label{TypedGSLTs}
% Talk about vm vs coarse graining

\subsection{Native type theory}
Dana Scott
% Scott, Dana, "Relating Theories of the Lambda-Calculus," in Seldin, J.P., and J.R. Hindley, eds., To H.B. Curry: Essays on Combinatory Logic, Lambda Calculus and Formalism, Academic Press, New York, 1980, pp. 403-450. https://www.andrew.cmu.edu/user/awodey/dump/Scott/ScottRelating.pdf
laid out a process for generating a type system from a category, on which Jacobs later much elaborated
% Categorical logic and type theory, Edited by Bart Jacobs, Studies in Logic and the Foundations of Mathematics, Volume 141, North—Holland (1999).
and Willams and Stay advocated:
% Christian Williams and Michael Stay, "Native Type Theory." Proceedings of the Fourth Annual Conference on Applied Category Theory, 2021. Published in EPTCS 372 (2022), 116–132.
given a category $C,$ calculate the internal language of the category ${\rm Set}^{C^{\rm op}}$ of presheaves on $C.$  The category of presheaves is a {\em topos}, a cartesian closed category with finite limits and a subobject classifier.  It is a setting in which to do intuitionistic logic: it allows us to talk about, among other things, product and arrow types; intersection, union, and implication of subtypes; and existential and universal quantification.  

The Yoneda embedding of $C$ into ${\rm Set}^{C^{\rm op}}$ is full and faithful, so the topos is itself a graph-structured lambda theory where we can think of the objects as types that refine the original sorts.

\subsection{A hypercube of type systems}

There are situations in which a smaller type system is more desirable. The lambda cube
% Barendregt, Henk (1991). "Introduction to generalized type systems". Journal of Functional Programming. 1 (2): 125–154.
is a collection of lambda calculi that include types as part of the term grammar.  A well-typed term is guaranteed to terminate.  

There is an unfortunate collision of jargon here: there is only one generating sort in the sense of a multisorted lambda theory, but Barendregt defines a sort to be either the constant $*,$ meaning ``type'', or the constant $\square,$ meaning ``kind'', and has an axiom $\vdash *\colon \square.$  He has two parametric inference rules for forming the dependent product type and for forming abstractions that depend on two sorts $s_1$ and $s_2.$  All of the calculi allow $s_1 = s_2 = *,$ but when we also allow $s_1, s_2,$ or both to be $\square,$ we get the three axes of the cube.  There is a functor that lets us construct an analogous hypercube of type systems from a GSLT.
% cite the hypercube functor paper

\section{GSLT to Rholang Compilation}

\subsection{Fine- and coarse-grained GSLTs}
In the RHO calculus, the terms form a commutative monoid.  However, there are $n!$ permutations of a term with $n-1$ parallel atomic processes; if we had to enumerate all the permutations until we found a term in the right form to reduce, it would be absurdly slow.  Even worse, the general problem of determining whether two words are in the same equivalence class is undecidable.  Actual implementations of process calculi will do things like replace variables with keys into a hash table, store all the receiving processes in buckets under their key, and then iterate over the sends looking for matches.  Therefore, when generating a vm and a type system from a GSLT, a language implementer will likely want to provide two GSLT presentations and a proof.

The first will be the fine-grained GSLT with all the optimizations and implementation details; it will only have equations pertaining to the source and target of rewrites and will have logic for things like reducing to a normal form or using a hash table.  We can derive a type system from such a GSLT, but it will likely be too complicated to be used by developers writing programs in the language.

The second will be a coarse-grained GSLT that abstracts away the implementation details and things like specific choices of normal form.  This is the GSLT that we feed to the algorithms above to generate a useful type system.

The proof should show that the fine-grained GSLT is a valid implementation of the coarse-grained one.  One way to do this is with a necessity-preserving functor between the free quivers on the reduction graphs.  Just due to being a functor, given a possible rewrite from one term to another in the coarse-grained GSLT, there is a path of possible rewrites between the corresponding terms in the fine-grained GSLT. Preserving necessity means that given a necessary rewrite from one term to another in the coarse-grained GSLT, there is a path of necessary rewrites between the corresponding terms in the fine-grained GSLT.

\subsection{Translation to Rholang}

Given a finitely-presented fine-grained GSLT, we would like to produce a Rholang program that implements it.  The general strategy is to send the state of the system on a channel, then produce a sum of ``edge'' processes that implement rewrites out of the state.  Each edge process in the sum attempts to consume the current state and produce the edge's target state.

\subsubsection{Enumerating rewrites}

Unfortunately, we can't produce a sum of all the edge processes simultaneously, due to a finite amount of memory.  In an implementation of the lambda calculus with a reduction strategy that allows beta reduction anywhere within a term, it is easy to produce an exponential number of possible rewrites out of a term.  For example, suppose we have a bunch of applications of the identity combinator to itself:

\begin{verbatim}
((I I) ((I I) ((I I) ((I I) ((I I) I)))))
\end{verbatim}

There are five different places in this lambda term where a beta reduction could occur, so there are $2^5 = 32$ different parallel reductions that could occur.

Even worse, consider the following rewrite rule we could add to RHO calculus: \bigskip

\noindent ${\rm Comm}(x, K, Q_1, Q_2): {\rm Send}(x, Q_1) \;|\; {\rm For}(x, K) \leadsto Q_2.$ \bigskip

\noindent This says that any interacting pair of processes can evolve to {\em any other term}!  It is useless computationally, but demonstrates that in principle we can have an infinite number of rewrites out of a source term.

We can write an interpreter with a process that unfolds the sum dynamically.  At each step, the interpreter may choose to synchronize with of the existing edge processes in the sum or to recurse and generate another edge process.

\begin{verbatim}
Intepreter = EdgeGen(state, 0)

EdgeGen(state, n) =
    EdgeProcess(state, n) + EdgeGen(state, n + 1)
\end{verbatim}

This technique produces a single-threaded interpreter that includes in its enumeration parallel rewrites on parts of the state.  It suffices for an interpreter, but does not make good use of the massive parallelization that RSpace provides.

\subsubsection{Adding parallelism}

A GSLT has a finite number of top-level and in-context rewrite constructors.  Top-level rewrites unify against a whole term, while in-context rewrites unify against part of a term and then recurse.  The recursion must terminate because the terms are of finite length.

However, the interpreter is not given rewrites, it's given a term that may be the source of a rewrite.  Matching the state agains the source of a rewrite constructor is a unification problem.

Given an in-context rewrite rule like ${\rm Par}_2$ that takes multiple rewrites as input, we can't just unify the state with that context and recurse.  All we know is that {\em if the substructure is the source of a rewrite} then we can apply the in-context rewrite rule.  We have to verify that the term is actually the source of the rewrite rule before adding it to the sum.  If we match the context first and then try to match the substructure, then in the context of a sum, it may win the race to be chosen but then fail to match the substructure.

Rholang supports garbage collection of private names.  For example, in the following Rholang process that uses the public name \verb+@Nil+, one of the sends wins the race and the losing send hangs around forever:

\begin{verbatim}
@Nil!(1) | @Nil!(2) | for (winner <- @Nil) stdout!(*winner)

    | 2 wins
    V

@Nil!(1) | stdout!(2)
\end{verbatim}

\noindent However, if we use private names, the loser can get garbage collected because we're guaranteed that no other process can synchronize with it:

\begin{verbatim}
new x in { x!(1) | x!(2) | for (winner <- x) stdout!(*winner) }

    | 2 wins
    V

new x in { x!(1) | stdout!(2) }

    || x not free in stdout!(2)

new x in { x!(1) } | stdout!(2)

    || no synchronization possible

Nil | stdout!(2)

    || monoid laws

stdout!(2)
\end{verbatim}

This design pattern allows us to write an interpreter that runs multiple interpreter threads in parallel whenever the pattern matches a rule like ${\rm Par}_2$ involving two parallel rewrites.  We still enumerate potential edge processes, but instead of a sum using Plus, we now have a race using Par.  In the sum case, the interpreter has to verify that the term is actually the left-hand side of a concrete rewrite before adding it to the sum.  In the race case, that verification is delegated to the potential-edge process itself: we can recurse on the structure of a term that matches an in-context rule without verifying that the substructure matches, because even if the substructure fails to match, it's not blocking other edge processes.

\section{RSpace}

The Rholang interpreter uses a very efficient data structure called \textbf{RSpace} for storing continuations and matching sends with receives.  

\subsection{RSpace on Mork}

foo
\end{document}
