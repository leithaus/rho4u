\documentclass{article}
\usepackage{amsmath, amssymb, hyperref}
\usepackage{graphicx}

\newcommand{\mycomment}[1]{}

\title{MeTTa Architecture Proposal}
\author{}
\date{}

\begin{document}

\maketitle

\section{Introduction}

SingularityNet's investment in MeTTa is predicated on the idea that intelligence opportunistically creates theories of computation to suit the domain in which the intelligence is operating.  In fact, the name MeTTa is intended as pun for a meta-level framework for specifying different theories of computation.  SingularityNet hopes that MeTTa will be a language in which one can describe a theory of computation and get an efficient interpreter or compiler that is suited for smart contracts on a blockchain~\cite{GoertzelMeredith2024}.

Without reproducing much of MeTTa's internals explicitly in the theory, MeTTa currently cannot model different evaluation strategies for lambda calculus, cannot model the semantics of pi calculi, and has several other problems preventing it from living up to this vision.  We propose an architecture based on category theoretical concepts that will enable MeTTa to:
\begin{itemize}
    \item specify the semantics of any discrete theory of computation
    \item generate an efficient compiler for that theory
    \item generate a sound type system customized to that theory
    \item support concurrency
    \item support transactions
\end{itemize}
and more.

\section{Theories of computation}

In this section, we attempt to implement a couple of theories of computation in MeTTa and uncover some missing features and some problems with the current implementation.

\subsection{The theory of lambda calculus}
MeTTa can model the operational semantics of certain virtual machines using a pattern similar to those of platforms like K Framework~\cite{kframework}.  Here's a formalization of the lambda calculus in MeTTa, with an evaluation strategy that reduces terms everywhere except under a lambda:

\begin{verbatim}
; -- The theory of lambda calculus

(: T Sort)
(: App (-> T T T))
(: Lam (-> (-> T T) T))
(= (App (Lam $f) $v) ($f $v)) ; Beta reduction

; -- Named functions due to lack of lambda in MeTTa

(: ident (-> T T))
(= (ident $x) $x)

(: omega (-> T T))
(= (omega $x) (App $x $x))

(: Omega (-> T T))
(= (Omega $x) (App (Lam omega) (Lam omega)))

; -- Example reductions

!(App X (App (Lam ident) Y))  ; [(App X Y)]
!(Lam Omega)                  ; [(Lam Omega)]
;!(App (Lam Omega) X)         ; infinite loop
\end{verbatim}

The theory introduces \verb+T+ as a sort, essentially the generator in the grammar.  It then declares the term constructors: \verb+App+ takes two terms and produces a term, while \verb+Lam+ takes a function from terms to terms and produces a term.  By using a function type, the theory bootstraps \verb+Lam+ by using the binders in MeTTa itself rather than modeling explicit substitution as part of the grammar.  MeTTa takes care of alpha equivalence of variable names internally.  Finally, the theory declares the beta rule.

Note that there's nothing in the theory corresponding to the rule defining the reduction strategy:

\[ \frac{{\tt T} \leadsto {\tt T'} \quad {\tt U} \leadsto {\tt U'}}{\mbox{\tt (App T U)} \leadsto \mbox{\tt (App T' U')}}. \]

The evaluations strategy is hardcoded into the MeTTa interpreter.  We could not, for instance, choose to reduce solely in head position:

\[ \frac{{\tt T} \leadsto {\tt T'}}{\mbox{\tt (App T U)} \leadsto \mbox{\tt (App T' U)}} \]

or to reduce under a lambda without considerably more machinery.

\subsection{The theory of RHO calculus}

Unfortunately, MeTTa is also unable to model the RHO calculus~\cite{MeredithRadestock2005}, a Reflective Higher-Order pi calculus, in a straightforward way:

\begin{verbatim}
(: P Sort)
(: N Sort)
(: Zero P)
(: Par (-> P P P))
(: Send (-> N P P))
(: Recv (-> N (-> N P) P))
(: At (-> P N))
(: Run (-> N P))
(= (Par (Send $chan $proc) (Recv $chan $cont)) ($cont (At $proc)))
(= (At (Run $n)) $n)
(= (Run (At $p)) $p)

; -- Problem: no support for structural equivalence.
; -- RHO terms form a commutative monoid, but in MeTTa
; -- the commutativity rewrite causes an infinite loop.

; (= (Par $p1 $p2) (Par $p2 $p1)) ; commutative
(= (Par $p Zero) $p) ; unital
(= (Par ((Par $p1 $p2) $p3) (Par $p1 (Par $p2 $p3)))) ; assoc.

; -- Helpers

(: nil (-> N P))
(= (nil $n) Zero)
(: chan (-> N))
(= (chan) (At Zero))

; -- Problem: reduction under a Send.
; -- According to RHO semantics, the printed process should
; -- not reduce, but MeTTa reduces it to (Send (At Zero) Zero).

(: Proc (-> P))
(= (Proc) (Par (Send (chan) Zero) (Recv (chan) nil)))
!(Send (chan) (Proc))
\end{verbatim}

\noindent MeTTa does well at modeling the sorts and term constructors.  There are sorts for Processes and Names of channels, five term constructors, and three rewrite rules.  But RHO calculus terms (as in all pi calculi) should be considered only up to structural congruence.  The term constructors form a commutative monoid under \verb+Par+, where \verb+Zero+ is the unit.  MeTTa has no concept of structural congruence.

One could argue that detecting whether two words are in the same equivalence class is, in general, undecidable, and any real implementation would use something like a normal form or a hash table to eliminate the detection problem.  The Rholang interpreter, in fact, uses such a strategy.  But the specific implementation details should not be part of the abstract description of the language.

RHO calculus---like all pi calculi---is a model of concurrent processes, and therefore can have races.  When multiple \verb+Send+s or \verb+Recv+s are competing on the same channel, RHO calculus makes a single nondeterministic choice of the winner of the race.  MeTTa takes all paths, so even very simple RHO calculus programs require exponential space and time to execute on the MeTTa interpreter.

Finally, RHO calculus also forbids reduction under a Send.  For example, the term 
\[\mbox{\tt(Send (chan) Proc)},\]
where
\[\mbox{\tt Proc = (Par (Send (chan) Zero) (Recv (chan) nil))}\]
should not reduce.  The process \verb+Proc+ will always reduce to \verb+Zero+ if it is permitted to do so; but under the RHO calculus semantics, \verb+Proc+ should be suspended until it is received and executed with the \verb+Run+ constructor.  This matters because when \verb+Proc+ is received, it could be executed in the context of another \verb+Recv+ that could interfere on \verb+(chan)+ and the two \verb+Recv+s would race to claim the \verb+Send+.

\subsection{Other theories}

Other missing features from MeTTa that impair its ability to implement theories of computation include:
\begin{itemize}
    \item the lack of a \verb+lambda+ grounded atom for defining functions inline
    \item no way to force evaluation of a subexpression
    \item no way to add facts to the database as the result of some computation
    \item a single shared fact database
    \item no way to have theories execute concurrently
    \item no transactions
\end{itemize}

\subsection{A way forward}

A model of computation generates a reduction graph.  The vertices of the graph are the possible states the computation can be in, and the edges are the possible state transitions.  We usually present a model of computation with two grammars, one for the states and the other for the transitions.


\mycomment{

    \begin{itemize}
        \item {\bf We can compile GSLTs to Rholang.} We outline a strategy for automatic compilation of GSLTs to Rholang in section \ref{comp_to_rholang}.
        \item {\bf We can generate a type system from a GSLT.} There are automatic ways to generate a type system from a category~\cite{WilliamsStay2021}. The generated type system allows for the expression of both spatial and behavioral properties of code that can be typechecked before interpretation.
        \item {\bf Application programming and concurrency.} Rholang seamlessly combines synchronous datatypes with the best-studied approach to concurrency, the $\pi$-calculus, to provide an efficient, transactional knowledge store.
        \item {\bf Reasoning about backtracking.} The MORK project has extracted the fact database into a blindingly efficient library called PathMap, with not only clear semantics but also clear complexity guarantees.  It lacks, however, any form of concurrency or notion of transaction.  We propose adding MORK to Rholang, both as a synchronous datatype as well as an augmentation of Rholang's pattern matching capabilities.
        \item {\bf Spaces remain isolated by default.} When using spaces as a datatype, they remain separate by default, just like maps, sets, and lists are.
        \item {\bf Rholang is optimal.}  In section \ref{optimality_proof} we prove that any other generic solution must be within a constant factor of the performance of Rholang.
    \end{itemize}

}

\section{Theories of Operational Semantics}

Our approach to operational semantics is to define a category $\mathsf{Th}$ whose generating objects are \emph{sorts}---the basic atoms out of which the state of the machine is assembled---and whose generating morphisms are term constructors that produce terms of those sorts.

Bill Lawvere pioneered the use of categories to capture grammars modulo equations for structural congruence, so such categories are often called \emph{Lawvere theories}. Lawvere proved that his theories correspond to finitary monads. Our theories are Lawvere theories equipped with extra structure and stuff that allow us to talk about reductions between terms, bind variables, and define modal types.  Specifically, they are \emph{graph-structured interactive multisorted lambda theories}, or GSLTs for short.

\begin{itemize}
    \item {\bf Multisorted.}  Lawvere only considered categories with one generating sort. We need at least two generating sorts, one for terms representing processes, and another for terms representing rewrites between processes. Todd Trimble gave a definition of multisorted Lawvere theories on the nLab~\cite{Trimble2018} and proved a similar monadicity theorem.

    \item {\bf Graph-structured.}  We distinguish one of these sorts, say $P$, as the sort of complete machine states, which we call processes. Rewrites are represented as a different sort $R$ equipped with source and target maps from $R$ to $P$. Let $\mathsf{Th(Gph)}$ be the category with two objects and two parallel morphisms between them. A graph-structured theory ${\rm Th}$ is one equipped with a functor from ${\rm Th}({\rm Gph})$ to ${\rm Th}$.

    \item {\bf Interactive.}  We restrict our attention to those programming languages where any base case rewrite rule---that is, any rewrite rule not constructed from another rewrite---is an occurrence of a binary process constructor $\odot,$ which we call the ``interaction''.  For example:

\begin{itemize}
    \item In lambda calculus, beta reduction is of the form $((\lambda x.T) U) \to T[U/x]$, whose source is an application of one process to another.  Here, $\odot$ is application.
    \item In pi calculus, the Comm rule is of the form $x?(y).P \mid x!(z) \to P[z/y]$, whose source is a juxtaposition of two processes.  Here, $\odot$ is juxtaposition.
\end{itemize}

An example of a rewrite where $\odot$ is not the top level term constructor occurs in lambda calculus, where a reduction may occur in the head position of an application term.  We have a rewrite that says given any rewrite $r$ and any term $x$, $s((r x)) = (s(r) x)$ and $t((r x)) = (t(r) x).$  Because these rewrites are constructed from $r,$ they are not subject to the requirement to have $\odot$ at the top level.

    \item {\bf Modalities from the interaction.}  Consider the type $[\odot([], A)] B$ of terms that when placed into a term context $\odot([], x)$ with any $x\colon A$ must reduce to a term of type $B$. In lambda calculus, this corresponds to the arrow type $A \to B$. In pi calculus, this is Caires' rely-guarantee type $A \triangleright B$~\cite{Caires2007}.  If $B$ can depend on $x$, these are dependent product types.  We can also define the dual notion of a possibility modality.

    \item {\bf Lambda theories.}  Lawvere's theories have finite products. While it is possible to define the machinery of bound variables and substitution in a Lawvere theory, it is a hassle and not particularly interesting. Instead, we use cartesian closed categories, which handle bound variables and substitution automatically.  Our lambda theories are multisorted Lawvere theories with a hom-product adjunction
$${\rm Th}(A \times B, C) \cong {\rm Th}(B, C^A).$$

\end{itemize}

\section{Presentation of finitely generated GSLTs}

A presentation of a GSLT consists of:

\begin{itemize}
    \item A finite set of \textbf{generating sorts}, including two distinguished sorts $P$ (processes) and $R$ (rewrites) from the graph structure.
    \item A choice of a distinguished binary morphism $\odot : X \times Y \to P$, called the \textbf{interaction}, for some pair of sorts $X, Y$.
    \item A finite set of \textbf{generating morphisms}, including the two built-in \textbf{graph structure morphisms} $s, t : R \to P$. Generating morphisms with codomain $P$ are \textbf{term constructors}, and those with codomain $R$ are \textbf{rewrite constructors}:
    \[
    r : \prod_i Z_i \to R
    \]
    If the domain does not contain a factor of $R$, it must factor through the interaction:
    \[
    \exists f : \prod_i Z_i \to X \times Y. \quad \odot \circ f = s \circ r
    \]
    \item A finite set of \textbf{equations} between morphisms. We often write $r : A \leadsto B$ as shorthand for $s(r(z)) = A$, $t(r(z)) = B$.
\end{itemize}

Rewrite term constructors usually come in two flavors:

\begin{itemize}
    \item \textbf{Top-level rewrites} whose domain does not contain a factor of $R$.
    \item \textbf{In-context rewrites} whose domain contains a factor of $R$.
\end{itemize}

\subsection{Examples}
\subsubsection{WHNF $\lambda-$calculus}

\begin{itemize}
    \item \textbf{Two generating sorts} $P, R$
    \item \textbf{Generating morphisms:}
    \begin{itemize}
        \item Graph structure morphisms: $s, t : R \to P$
        \item Term constructors: 
        \begin{itemize}
            \item ${\tt App} : P \times P \to P$
            \item ${\tt Lam} : (P \to P) \to P$
        \end{itemize}
        \item Rewrite constructors:
        \begin{itemize}
            \item ${\tt Beta} : (P \to P) \times P \to R$
            \item ${\tt Head} : R \times P \to R$
        \end{itemize}
    \end{itemize}
    \item \textbf{Interaction:} ${\tt App}$
    \item \textbf{Equations:}
    \begin{itemize}
        \item ${\tt Beta}(A, B) : {\tt App}({\tt Lam}(A), B) \leadsto ev(A, B)$ (top-level)
        \item ${\tt Head}(A, B) : {\tt App}(s(A), B) \leadsto {\tt App}(t(A), B)$ (in-context)
    \end{itemize}
\end{itemize}

\subsubsection{RHO calculus}

\begin{itemize}
    \item \textbf{Three generating sorts} $P, R, N$
    \item \textbf{Generating morphisms:}
    \begin{itemize}
        \item Graph structure morphisms: $s, t : R \to P$
        \item Term constructors: 
        \begin{itemize}
            \item ${\tt Zero} : 1 \to P$
            \item ${\tt Par} : P \times P \to P$
            \item ${\tt Send} : N \times P \to P$
            \item ${\tt For} : N \times (N \to P) \to P$
            \item ${\tt Drop} : N \to P$
            \item ${\tt Quote} : P \to N$
        \end{itemize}
        \item Rewrite constructors:
        \begin{itemize}
            \item ${\tt Comm} : N \times (N \to P) \times P \to R$
            \item ${\tt Par}_1 : R \times P \to R$
            \item ${\tt Par}_2 : R \times R \to R$
        \end{itemize}
    \end{itemize}
    \item \textbf{Interaction:} ${\tt Par}$
    \item \textbf{Equations:}
    \begin{itemize}
        \item Commutative monoid laws
            \begin{itemize}
                \item ${\tt Par}({\tt Zero}, Q) = Q$
                \item ${\tt Par}(T, U) = {\tt Par}(U, T)$
                \item ${\tt Par}({\tt Par}(T, U), V) = {\tt Par}(T, {\tt Par}(U, V))$
            \end{itemize}
        \item Quotation laws
            \begin{itemize}
                \item ${\tt Quote}({\tt Drop}(x)) = x$
                \item ${\tt Drop}({\tt Quote}(Q)) = Q$ (but see section \ref{TypedGSLTs})
            \end{itemize}
        \item Communication
            \begin{itemize}
                \item ${\tt Comm}(x, K, Q) : {\tt Par}({\tt For}(x, K), {\tt Send}(x, Q)) \leadsto ev(K, @Q)$
            \end{itemize}
        \item Par contexts
            \begin{itemize}
                \item ${\tt Par}_1(r, Q) : {\tt Par}(s(r), Q) \leadsto {\tt Par}(t(r), Q)$
                \item ${\tt Par}_2(r_1, r_2) : {\tt Par}(s(r_1), s(r_2)) \leadsto {\tt Par}(t(r_1), t(r_2))$
            \end{itemize}
    \end{itemize}
\end{itemize}

\section{Generating typed GSLTs from untyped ones}
\label{TypedGSLTs}
% Talk about vm vs coarse graining

\subsection{Native type theory}
Dana Scott ~\cite{Scott1980} laid out a process for generating a type system from a category, on which Jacobs later much elaborated ~\cite{Jacobs1999} and Willams and Stay advocated~\cite{WilliamsStay2021}:
given a category $C,$ calculate the internal language of the category ${\rm Set}^{C^{\rm op}}$ of presheaves on $C.$  The category of presheaves is a {\em topos}, a cartesian closed category with finite limits and a subobject classifier.  It is a setting in which to do intuitionistic logic: it allows us to talk about, among other things, product and arrow types; intersection, union, and implication of subtypes; and existential and universal quantification.  

The Yoneda embedding of $C$ into ${\rm Set}^{C^{\rm op}}$ is full and faithful, so the topos is itself a graph-structured lambda theory where we can think of the objects as types that refine the original sorts.

\subsection{A hypercube of type systems}

There are situations in which a smaller type system is more desirable. The lambda cube~\cite{Barendregt1991} is a collection of lambda calculi that include types as part of the term grammar.  A well-typed term is guaranteed to terminate.  

There is an unfortunate collision of jargon here: there is only one generating sort in the sense of a multisorted lambda theory, but Barendregt defines a sort to be either the constant $*,$ meaning ``type'', or the constant $\square,$ meaning ``kind'', and has an axiom $\vdash *\colon \square.$  He has two parametric inference rules for forming the dependent product type and for forming abstractions that depend on two sorts $s_1$ and $s_2.$  All of the calculi allow $s_1 = s_2 = *,$ but when we also allow $s_1, s_2,$ or both to be $\square,$ we get the three axes of the cube.  There is a functor that lets us construct an analogous hypercube of type systems from a GSLT~\cite{HypercubePaper}.

\section{GSLT to Rholang Compilation}

\subsection{Fine- and coarse-grained GSLTs}
In the RHO calculus, the terms form a commutative monoid.  However, there are $n!$ permutations of a term with $n-1$ parallel atomic processes; if we had to enumerate all the permutations until we found a term in the right form to reduce, it would be absurdly slow.  Even worse, the general problem of determining whether two words are in the same equivalence class is undecidable.  Actual implementations of process calculi will do things like replace variables with keys into a hash table, store all the receiving processes in buckets under their key, and then iterate over the sends looking for matches.  Therefore, when generating a vm and a type system from a GSLT, a language implementer will likely want to provide two GSLT presentations and a proof.

The first will be the fine-grained GSLT with all the optimizations and implementation details; it will only have equations pertaining to the source and target of rewrites and will have logic for things like reducing to a normal form or using a hash table.  We can derive a type system from such a GSLT, but it will likely be too complicated to be used by developers writing programs in the language.

The second will be a coarse-grained GSLT that abstracts away the implementation details and things like specific choices of normal form.  This is the GSLT that we feed to the algorithms above to generate a useful type system.

The proof should show that the fine-grained GSLT is a valid implementation of the coarse-grained one.  One way to do this is with a necessity-preserving functor between the free quivers on the reduction graphs.  Just due to being a functor, given a possible rewrite from one term to another in the coarse-grained GSLT, there is a path of possible rewrites between the corresponding terms in the fine-grained GSLT. Preserving necessity means that given a necessary rewrite from one term to another in the coarse-grained GSLT, there is a path of necessary rewrites between the corresponding terms in the fine-grained GSLT.

\subsection{Compilation to Rholang}
\label{comp_to_rholang}

Given a finitely-presented fine-grained GSLT, we would like to produce a Rholang program that implements it.  The general strategy is to send the state of the system on a channel, then produce a sum of ``edge'' processes that implement rewrites out of the state.  Each edge process in the sum attempts to consume the current state and produce the edge's target state.

\subsubsection{Enumerating rewrites}
\label{enum_rewrites}

We can't produce a sum of all the edge processes out of a state simultaneously due to a finite amount of memory.  In an implementation of the lambda calculus with a reduction strategy that allows beta reduction anywhere within a term, it is easy to produce an exponential number of possible rewrites out of a term.  For example, suppose we have a bunch of applications of the identity combinator \verb+I = +$\lambda x.x$ to itself:

\begin{verbatim}
((I I) ((I I) ((I I) ((I I) ((I I) I)))))
\end{verbatim}

There are five different places in this lambda term where a beta reduction could occur, so there are $2^5 = 32$ different parallel reductions that could occur.

Even worse, consider the following modification of RHO calculus' Comm rule: \bigskip

\noindent ${\rm Comm}(x, K, Q_1, Q_2): {\rm Send}(x, Q_1) \;|\; {\rm For}(x, K) \leadsto Q_2.$ \bigskip

\noindent This says that any interacting pair of processes can evolve to {\em any other term}!  It is useless computationally, but demonstrates that in principle we can have an infinite number of rewrites out of a source term.

We can write an interpreter with a process that unfolds the sum dynamically.  At each step, the interpreter may choose to synchronize with of the existing edge processes in the sum or to recurse and generate another edge process.

\begin{verbatim}
Intepreter = EdgeGen(state, 0)

EdgeGen(state, n) =
    EdgeProcess(state, n) + EdgeGen(state, n + 1)
\end{verbatim}

This technique produces a single-threaded interpreter that includes in its enumeration parallel rewrites on parts of the state.  It suffices for an interpreter, but does not make good use of the massive parallelization that RSpace provides.

\subsubsection{Adding parallelism}

A GSLT has a finite number of top-level and in-context rewrite constructors.  Top-level rewrites unify against a whole term, while in-context rewrites unify against part of a term and then recurse.  The recursion must terminate because the terms are of finite length.

However, the interpreter is not given rewrites, it's given a term that may be the source of a rewrite.  Matching the state agains the source of a rewrite constructor is a unification problem.  The implementation above needs to verify that the entire term is the source of a rewrite in order to add that edge process to the sum.  Rather than preverifiy and sum the resulting processes, we can delegate the verification to processes that are {\em potentially} edges out of the state and simply run them in parallel.  Any potential edge process that manages to verify that the current state really is the source of that edge can then send a message to a channel waiting for a winner.

RHO calculus only has public names, so if we tried this technique in RHO calculus, it would leave a lot of garbage laying around:

\begin{verbatim}
@Nil!(1) | @Nil!(2) | for (winner <- @Nil) stdout!(*winner)

    | 2 wins
    V

@Nil!(1) | stdout!(2)
\end{verbatim}

\noindent However, Rholang also allows private names, like those used by pi calculus.  If we use private names, the loser can get garbage collected because we're guaranteed that no other process can synchronize with it:

\begin{verbatim}
new x in { x!(1) | x!(2) | for (winner <- x) stdout!(*winner) }

    | 2 wins
    V

new x in { x!(1) | stdout!(2) }

    || x not free in stdout!(2)

new x in { x!(1) } | stdout!(2)

    || no synchronization possible

Nil | stdout!(2)

    || monoid laws

stdout!(2)
\end{verbatim}

This design pattern allows a much simpler interpreter that has one potential edge process for each rewrite constructor in the GSLT.  If an in-context rewrite constructor matches the state, the interpreter forks new processes that independently try to verify that the substructure is the source of a rewrite, then joins those processes and signals that it found an actual edge.

\subsubsection{Detecting optimization opportunities}

There are often more opportunities for optimization, but they only apply in certian circumstances.  Detecting opportunities for optimization is a standard part of compiler design, and will be important for a compilation pipeline targeting Rholang.  For example, in a programming language with functions and an eager reduction strategy, it doesn't make sense to decompose the state from the top level at each step; instead, subexpressions should be reduced completely before moving up in the syntax tree.  Standard heuristics can be brought to bear on the sets of rewrite constructors, and automated techniques exist for generalizing specific instances of optimizations to the most general applicable situation~\cite{Tate2010}.

\section{RSpace}

The Rholang interpreter uses a very efficient data structure called \textbf{RSpace} for storing continuations and matching sends with receives.  To a first approximation, the Rholang interpreter is just a parser sitting on top of RSpace.

In the simplest cases, the names on which processes are sending and receiving get hashed, and if there's a process of the opposite polarity waiting in the hash table, they can synchronize.  RSpace also allows processes to bind variables using patterns.  This lets programmers dispatch messages on the same channel to different processes based on the content of the messages, and is the basis of the design of the interpreters above.

\subsection{RSpace and MORK}

MORK (MeTTa Optimal Reduction Kernel) is a powerful functional language for manipulating graph databases in memory. It represents sets of paths using a trie, which can be exponentially smaller than the original dataset.  It can also act on all the paths in a subtrie simultaneously, which can be exponentially faster than mapping over a data set.  But it does not have any transactional semantics or primitives for concurrency.  As such, RSpace and MORK are complementary languages, well-suited to each other.  Rholang 1.2 will embed MORK as a datatype in the same way that it embeds numbers, strings, lists, maps, and sets.  

\section{Optimality}


\section{Conclusion}

\begin{thebibliography}{99}

\bibitem{Barendregt1991}
Barendregt, Henk (1991). 
``Introduction to generalized type systems.''  
\emph{Journal of Functional Programming}, 1(2): 125--154.

\bibitem{Caires2007}
Caires, Luis (2007). 
``Logical Semantics of Types for Concurrency.'' 
In \emph{Algebra and Coalgebra in Computer Science}, 
Second International Conference, CALCO 2007, Bergen, Norway, August 20--24, 2007, Proceedings.
\url{http://ctp.di.fct.unl.pt/~lcaires/papers/CALCO-Caires-1.0.pdf}

\bibitem{GoertzelMeredith2024}
Goertzel, Ben, and L. Gregory Meredith. Private communication.

\bibitem{HackerNews}
Hacker News (2017). 
\url{https://news.ycombinator.com/item?id=14439137}.

\bibitem{HypercubePaper}
[Placeholder for the Hypercube functor paper reference; “cite the hypercube functor paper”].

\bibitem{Jacobs1999}
Jacobs, Bart (1999).  
\textit{Categorical Logic and Type Theory}.  
Studies in Logic and the Foundations of Mathematics, Vol.\ 141.  
North-Holland.

\bibitem{kframework}
K Framework.
\url{http://kframework.org}.

\bibitem{Lawvere1963}
Lawvere, William (1963). 
``Functorial Semantics of Algebraic Theories'' (PhD Thesis). 
\url{http://www.tac.mta.ca/tac/reprints/articles/5/tr5abs.html}.

\bibitem{McCarthy1978}
McCarthy, John (1978). 
``History of LISP.'' 
Presented at the \textit{ACM SIGPLAN History of Programming Languages Conference (HOPL), June 1--3, 1978}.  
Published in \textit{History of Programming Languages},  
edited by Richard Wexelblat,  
Academic Press, 1981.

\bibitem{MeredithRadestock2005}
Meredith, L.~Gregory, and Matthias Radestock (2005). 
``A Reflective Higher-order Calculus.'' 
\emph{Electronic Notes in Theoretical Computer Science} 141(5), 49--67, 22 December 2005. 
\url{https://doi.org/10.1016/j.entcs.2005.05.016}

\bibitem{Scott1980}
Scott, Dana (1980). 
``Relating Theories of the Lambda-Calculus.'' 
In Seldin, J.P., and J.R. Hindley (eds.), 
\textit{To H.B. Curry: Essays on Combinatory Logic, Lambda Calculus and Formalism},  
pp.\ 403--450. Academic Press, New York.  
\url{https://www.andrew.cmu.edu/user/awodey/dump/Scott/ScottRelating.pdf}.

\bibitem{Tate2010}
Tate, Ross, Michael Stepp, and Sorin Lerner (2010). 
``Generating Compiler Optimization from Proofs.'' 
In \emph{POPL '10: Proceedings of the 37th annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}, 
pp.\ 389--402, Madrid, Spain. 
ACM, New York, NY, USA. 
\url{http://www.cs.cornell.edu/~ross/publications/proofgen/}.

\bibitem{tiobeIndex}
TIOBE Index (2025). 
\url{https://web.archive.org/web/20250106055905/www.tiobe.com/tiobe-index/}.

\bibitem{Trimble2018}
Trimble, Todd (2018). 
``Multisorted Lawvere Theories,'' on the nLab. 
\url{https://ncatlab.org/toddtrimble/published/multisorted+Lawvere+theories}.

\bibitem{WilliamsStay2021}
Williams, Christian, and Michael Stay (2021). ``Native Type Theory.'' \emph{Proceedings of the Fourth Annual Conference on Applied Category Theory}. EPTCS 372 (2022), 116--132.

\end{thebibliography}
\end{document}
